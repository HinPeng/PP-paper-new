%!TEX root = main.tex
\section{Introduction}
\IEEEPARstart{D}{eep} neural networks (DNNs) have achieved remarkable success across various fields,
including computer vision, autonomous driving, and particularly in natural language processing (NLP),
as demonstrated by models like GPT-4~\cite{achiam2023gpt} and Llama3~\cite{dubey2024llama}.
The impressive performance of DNNs stems from advancements in their architecture
and the growing size of their parameters.
However, there is a significant disparity between the exponential growth in model parameters
and the slower increase in GPU memory capacity.
As a result, training such large models exceeds the memory limitations of a single GPU.
Consequently, training DNNs across multiple GPUs has become increasingly
prevalent in both academia~\cite{miao2023hetu,zhengAlpaAutomatingInter2022} and industry~\cite{abadiTensorFlowSystemLargeScale2016,paszkePytorchImperativeStyle2019}.

There are three primary parallelism methods for training large-scale DNNs:
\emph{Data parallelism (DP)}, \emph{Tensor parallelism (TP)}, and \emph{Pipeline parallelism (PP)}.
\emph{DP} divides the training data into multiple shards,
distributing them across different devices, with each device storing a complete copy of the model parameters.
During each training iteration, all devices must communicate to synchronize and update the model parameters.
\emph{TP} distributes computations across multiple GPUs by partitioning tensors along specific dimensions,
enabling simultaneous processing of different parts of the neural network.
While both \emph{DP} and \emph{TP} effectively distribute memory and computation across devices,
\emph{DP} faces significant communication overhead as the model size increases,
while \emph{TP} struggles with synchronization delays due to inter-device communication.

In contrast, \emph{pipeline parallelism} aims to parallelize computations across the layers of DNNs.
It splits DNNs into multiple layer blocks and divides the input data of a batch into smaller micro-batches,
enabling pipelined execution across different stages.
Communication occurs only between adjacent stages and can be overlapped with ongoing computations,
leading to lower communication volume compared to \emph{DP} and lower bandwidth requirements compared to \emph{TP}.
Nonethless, achieving both high computation efficiency and memory utilization remains a significant challenge in \emph{PP}.

Current pipeline parallelism frameworks can be classified into two categories:
\emph{Synchronous Pipeline Parallelism (SPP)} and \emph{Asynchronous Pipeline Parallelsim (APP)}.
\emph{SPP} typically suffers from pipeline bubble overhead,
while \emph{APP} requires storing multiple versions of model parameters
to maintain consistency within the same micro-batch.
Both approaches aim to balance the execution time across stages,
as any lagging (straggler) stage causes delays in the entire pipeline,
slowing down overall training performance.
However, focusing solely on balancing computation time often overlooks memory usage at different stages.
We have observed over 40\% GPU memory waste in existing pipeline parallel training frameworks.
This waste stems from two factors: 1) the varying ratio of computation time to memory usage across different types of layers,
and 2) the inconsistency in the number of parameter versions, activation memory, and gradients required at each stage,
further exacerbating memory imbalance.
This imbalance significantly impacts the scalability of pipeline parallelism with respect to memory.

\emph{Memory swapping}~\cite{rhuVDNNVirtualizedDeep2016,jinLayerCentricMemoryReuse2018,wangSuperneuronsDynamicGPU2018,huangSwapAdvisorPushingDeep2020,renSentinelEfficientTensor2021}
and \emph{recomputation}~\cite{chenTrainingDeepNets2016,kirisameDynamicTensorRematerialization,heHOMEHolisticGPU2022,korthikantiReducingActivationRecomputation2023}
are two popular techniques for reducing memory usage during DNN training.
\emph{Swapping} leverages CPU memory as an external buffer to extend GPU memory capacity,
while \emph{recomputation} discards activation memory after forward propagation,
regenerating it later through redundant computation.
Both methods influence the memory footprint and execution time of a DNN,
creating a vast and complex search space when combined with pipeline parallel partitioning.
This combined optimization is an NP-hard problem due to its exponential search complexity.

All prior works address pipeline parallel partitioning and memory optimization at a coarse granularity,
typically at the layer or layer block level.
While this approach simplifies partitioning, it limits the optimization space and increases overhead.
In contrast, finer-grained, \emph{tensor-level} optimization methods have proven
more efficient in prior memory optimization studies~\cite{pengCapuchinTensorbasedGPU2020,zhangTENSILETensorGranularity2022,nieTSPLITFinegrainedGPU2022}.
Additionally, these methods rewrite the user-submitted model code at the layer (block)
level to enable code generation for each stage.
However, this approach lacks generalizability in production environments,
particularly when user models involve custom modules or when privacy concerns
prevent third-party developers from accessing the model structure.

In this paper, we propose DawnPiper, a memory-scalable pipeline parallelism partitioning method
designed to enhance both pipeline training performance and GPU memory utilization.
Firstly, we develop a DL compilation-based profiling technique
to generate a fine-grained computation graph for the model.
This expanded partition space allows for precise adjustments to the computation time and memory footprint of each stage.
Additionally, the module code for each stage is automatically generated using DL compilation technique.
Profiling results from typical DNN training reveal two key memory usage characteristics:
the majority (over 90\%) of activation and operation memory consumption is relatively small (around 100 MB).
Building on this, we derive a pipeline parallel partitioning theorem that defines
the partition range between compute-balanced and memory-balanced points when dividing adjacent pipeline stages.

Secondly, building on the proposed pipeline partition theorem,
we introduce a binary pipeline parallel partition algorithm.
Starting from the middle stage,
we treat the left and right model segments as two adjacent stages
and recursively explore potential partition points between the compute-balanced and memory-balanced points.
For each partition plan, we apply a cost-model-based memory optimization method,
as proposed by Capuchin~\cite{pengCapuchinTensorbasedGPU2020}, to minimize memory usage within GPU capacity in linear time.
Then we can record the computation time of the longest execution stage.
By evaluating all partition plans, the strategy that results in the shortest execution time for the longest stage
is identified as the (nearly) optimal partition solution.

The main contributions of DawnPiper are as follows:
\begin{itemize}
      \item We introduce a DL compilation-based model profiling method that enables
      fine-grained control over pipeline partitioning and memory optimization,
      while also supporting automatic code generation for each pipeline stage.
      \item Leveraging the detailed profiling,
      we conducted an in-depth analysis of memory usage during model training and
      identified two key memory usage characteristics.
      Based on these findings, we propose a pipeline partition theorem that defines
      the partition range between compute-balanced and memory-balanced points.
      \item Building on this theorem,
      we propose a binary pipeline partition algorithm that efficiently searches
      for near-optimal partitioning and memory optimization strategies within the constraints of GPU memory capacity.
      \item We have prototyped DawnPiper on PyTorch~\cite{paszkePytorchImperativeStyle2019}.
      Our evaluations demonstrate that DawnPiper achieves up to a 4$\times$ and 11$\times$ increase
      in maximum trainable batch size compared to vPipe~\cite{zhaoVPipeVirtualizedAcceleration2022}
      and PipeDream~\cite{narayananPipeDreamGeneralizedPipeline2019}, respectively,
      and offers up to a 1.5$\times$ performance speedup compared to vPipe.
\end{itemize}

The rest of the this paper is organized as follows.
Section~\ref{sec:background} introduces the background of
the pipeline parallelism and two memory optimization techniques.
Section~\ref{sec:cam} presents the design challeges and motivations of DawnPiper.
Section~\ref{sec:design} and \ref{sec:imp} give the design overview and
implementation of DawnPiper.
Section~\ref{sec:evaluation} evaluates the performance of DawnPiper.
Section~\ref{sec:related} discusses the related works
and Section~\ref{sec:conclusion} concludes this paper.
\label{sec:intro}
