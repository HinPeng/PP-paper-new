%!TEX root = main.tex
\begin{abstract}
Pipeline parallelism is a crucial paradigm for large-scale model training.
However, imbalances in memory footprint across stages can lead to significant GPU memory wastage,
limiting the model sizes that pipeline parallelism can effectively support.
In this paper, we introduce DawnPiper, a memory-scalable pipeline parallel training framework. 
Firstly, we develop a DL compilation-based profiling method that transforms the model into a fine-grained computation graph.
This refinement gives us a finer granularity of model partitioning and memory optimization
while facilitating automatic code generation.
Based on observed memory usage characteristics,
we derive a pipeline parallelism partition theorem that effectively reduces the partition search space. 
Secondly, we propose a binary pipeline partitioning algorithm and
utilize a cost-model based memory optimization approach to
efficiently identify nearly optimal pipeline parallel strategy.
DawnPiper achieves up to a 4$\times$ and 11$\times$ increase in trainable maximum batch size compared to vPipe and PipeDream, respectively,
and provides up to a 1.5$\times$ performance speedup compared to vPipe.
\end{abstract}
