%!TEX root = main.tex
\begin{abstract}
Pipeline parallelism is a important parallelism paradigm for large-scale model training.
However, there is a huge GPU memory wasting ,
which 
In this paper, we propose DawnPiper,
a memory scablable pipeline parallel training framework.
Firstly, we develop a DL compilation based profiling method to transform
the submitted model to a fine-grained computation graph, 
which can refine the granularity of the model partition and memory optimization,
along with facilitating the automatic code generation.
Based on the observation on the memory usage characteristics,
we derive a pipeline parallelism partition theorem,
which can effectively reduce the partition search space.
Secondly, we propose a binary pipeline partition algorithm
.
We can .
DawnPiper can accomplish up to 4$\times$ and 11$\times$ increase on trainable maximum batch size
compared to vPipe and PipeDream, and up to 1.5$\times$ performance speedup compared to vPipe.
\end{abstract}
