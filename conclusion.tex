%!TEX root = main.tex
% \section{Conclusions and Future Works}
\section{Conclusions}
\label{sec:conclusion}
This paper introduces DawnPiper,
a memory-scalable pipeline parallel training framework
designed to achieve efficient pipeline parallelism while minimizing GPU memory resource waste.
The framework begins by compiling and profiling the model to
obtain a fine-grained computation graph and detailed memory usage characteristics,
which refines the model partitioning and memory optimization space.
Based on observed memory usage features during training,
DawnPiper employs a binary pipeline parallel partitioning method with partition range limitations,
combined with a cost-model based memory optimization method.
% This approach significantly reduces the search space for pipeline parallel partitioning.
Experimental results on four typical DNNs demonstrate that
DawnPiper increases the trainable batch size by an average of 1.29$\times$ and 1.71$\times$ compared to GPipe and vPipe, respectively,
and up to 4.8$\times$â€“11$\times$ compared to PipeDream.
Additionally, it accelerates the parallel training speed of vPipe by up to 1.5$\times$.