%!TEX root = main.tex
\section{Related Work}
\label{sec:related}
\subsection{Data Parallelism}
Data parallelism is a widely used parallel strategy
that divides the training data into multiple shards
and distributes them across different devices.
As model parameters and optimizer states increasingly consume
a significant portion of memory during NLP model training,
Ren et al. introduced the Zero Redundancy Optimizer (ZeRO)~\cite{rajbhandariZeROMemoryOptimizations2020}
to evenly distribute this memory across devices.
ZeRO-Offload~\cite{renZerooffloadDemocratizingBillionScale2021} advances this approach
by offloading all parameters and optimizer computations to the CPU,
and redesigns the CPU optimizer computation method to accelerate the process.
ZeRO-Infinity~\cite{rajbhandariZeROinfinityBreakingGPU2021} builds on ZeRO-Offload
by further optimizing memory usage through offloading both the optimizer and GPU memory to the CPU,
non-volatile memory (NVM), and other external storage, thereby expanding available GPU memory.
PatrickStar~\cite{fangParallelTrainingPreTrained2022} proposed organizing memory in blocks
and using these blocks as communication units to enhance communication bandwidth utilization.
In general, data parallelism is orthogonal to our work but can be used in conjunction for model training.

\subsection{Pipeline and Hybrid Parallelism}
GPipe~\cite{huangGpipeEfficientTraining2019} and PipeDream~\cite{narayananPipeDreamGeneralizedPipeline2019}
are pioneering works in synchronous and asynchronous pipeline parallelism, respectively.
DAPPLE~\cite{fanDAPPLEPipelinedData2021} introduces the 1F1B computation scheduling into synchronous pipeline parallelism
by dividing the batch into smaller micro-batches during the initial warm-up phase.
It reduces pipeline bubbles at the cost of requiring a sufficient number of micro-batches,
which is not always feasible for NLP models.
Chimera~\cite{liChimeraEfficientlyTraining2021} proposes a bidirectional synchronous pipeline scheduling method,
reducing pipeline bubbles by enabling a GPU to handle both forward and backward computations of different stages.
Hanayo~\cite{liuHanayoHarnessingWavelike2023} combines DAPPLE and Chimera’s approaches,
introducing a wave-based computation scheduling method.
Narayanan et al.~\cite{narayananEfficientLargeScaleLanguage2021} mitigate pipeline bubbles by
allowing a stage’s GPU to store more model partitions, based on DAPPLE’s scheduling,
though this increases communication overhead.
Qi et al.~\cite{qiZeroBubbleAlmost} achieves nearly bubble-free
pipeline parallelism by re-scheduling backward computation.
BPipe~\cite{kimBPipeMemoryBalancedPipeline2023} balances GPU memory usage
between stages by asynchronously exchanging activations between GPUs.
vPipe~\cite{zhaoVPipeVirtualizedAcceleration2022} uses an iterative algorithm to
dynamically balance model partitioning, memory swapping, and recomputation, employing the Kernighan-Lin algorithm~\cite{kernighanEfficientHeuristicProcedure1970}.
AdaPipe~\cite{sunAdaPipeOptimizingPipeline2024} introduces a two-step
dynamic programming approach for pipeline partitioning and recomputation.
Pipeline parallelism is often used in conjunction with tensor parallelism,
as seen in hybrid parallelism frameworks like Flexflow~\cite{jiaDataModelParallelism2019} and Alpa~\cite{zhengAlpaAutomatingInter2022}.
We plan to explore the extension of DawnPiper to hybrid parallelism in future work.