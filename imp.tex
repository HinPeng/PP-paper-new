%!TEX root = main.tex
\section{Implementation}
\label{sec:imp}
In this section, we will introduce the process for compiling and profiling a model in PyTorch,
as well as the implementation of memory swapping and recomputation.
Although these implementations are tailored to PyTorch,
the methods proposed in this paper are broadly applicable to other deep learning frameworks,
such as TensorFlow~\cite{abadiTensorFlowSystemLargeScale2016}.

\subsection{Compilation and Profiling}
\textbf{Compilation:} The purpose of model compilation
is to generate a fine-grained computation graph,
which expands the model partitioning and memory optimization space
while enabling automatic code generation for each pipeline stage.
Unlike tensor parallelism, this process does not involve splitting and tracking operations within individual operators.
To achieve this, we leverage the \emph{torch.fx} library~\cite{reedTorchFxPractical2022},
a pure Python toolkit provided by PyTorch for capturing and transforming neural network structures via tracing.
\emph{torch.fx} traces the model defined by user scripts without
dissecting internal computations within \texttt{nn.Module} operators.
Users can modify the captured computation graph freely,
and \emph{torch.fx} can then generate the corresponding Python code based on the updated graph.

\textbf{Profiling:} To profile the forward and backward computation times of nodes after compilation,
we insert timing hook functions at the start and end of the respective computations.
The initial training iterations during the warmup phase are excluded,
and the average is calculated over 50 batches of stable iterations.
For capturing the memory sizes of activations, parameters, and optimizer states,
we pass proxy input tensors of equivalent size through the computation graph and execute it once.
The corresponding memory sizes are extracted from the result tensors once each node’s computation completes.
Additionally, the saved tensor information for each node is retrieved during this execution
using the \texttt{torch.autograd.graph.saved\_tensors\_hooks} function provided by PyTorch, which we register for each node.


\subsection{Swap and Recomputation}
\textbf{Memory swap:} PyTorch does not natively support memory swapping operations.
Although a tensor can be transferred to a target device using the \texttt{Tensor.to(device)} interface,
its underlying GPU memory cannot be released without deleting the tensor object after the transfer completes.
To address this, we modified the underlying source code of PyTorch’s tensor data structure,
adding two functions: one that directly releases the underlying GPU memory without deleting the tensor object,
and another that sets the underlying memory address to a specified parameter.
This modification allows the original memory to be released after swapping out,
while setting the original tensor’s memory address to that of the swapped-in tensor.
For triggering memory swapping operations,
we use PyTorch's hook registration function to insert the operations at the appropriate points before training begins.
Once set, these memory swaps are consistently executed according to the defined strategy during pipeline parallel training.

\textbf{Recomputation:} PyTorch offers the \texttt{torch.utils.checkpoint} library for
handling recomputation by rewriting the parts of the module code that require it.
This library initiates recomputation when the corresponding backward computation starts,
functioning as on-demand recomputation.
Since we do not need another GPU stream for early recomputation process,
the library’s functionality is more than adequate for our needs.
Additionally, the module code for recomputation within each stage is
automatically generated during the module code generation phase.
